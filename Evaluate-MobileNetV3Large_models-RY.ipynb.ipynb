{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c444d11-8ab1-4611-85d2-5532253d0f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.models as models\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe551b04-94bd-4a8c-a93b-4f048a81bee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9431a6a7-7037-4c61-8967-ef557383c4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "############### Data Loader Img (or Depth / 3channel) for RY ###############\n",
    "##################################################################################\n",
    "class VideoDataset3ch(Dataset):\n",
    "    def __init__(self, orig_root_dir, transform=None, num_frames=16, is_train=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            orig_root_dir (str): Path to the root directory containing the original video files.\n",
    "            depth_root_dir (str): Path to the root directory containing the corresponding depth video files.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            num_frames (int, optional): Number of frames to be sampled from each video.\n",
    "            is_train (bool, optional): Flag to indicate if the dataset is used for training.\n",
    "        \"\"\"\n",
    "        self.orig_root_dir = orig_root_dir\n",
    "        self.transform = transform\n",
    "        self.num_frames = num_frames\n",
    "        self.is_train = is_train\n",
    "        self.classes = ['attack', 'real']  # Label mapping: 0 = attack, 1 = real\n",
    "        self.samples = self._load_samples()\n",
    "\n",
    "    def _load_samples(self):\n",
    "        \"\"\"\n",
    "        Load paths to original videos and their corresponding depth videos, along with labels.\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        for cls in self.classes:\n",
    "            orig_cls_dir = os.path.join(self.orig_root_dir, cls)\n",
    "            for root, _, files in os.walk(orig_cls_dir):\n",
    "                for fname in files:\n",
    "                    if fname.endswith(('.mp4', '.mov', '.avi')):\n",
    "                        orig_video_path = os.path.join(root, fname)\n",
    "                        samples.append((orig_video_path, self.classes.index(cls)))\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _load_frames(self, video_path, frame_indices, is_depth=False):\n",
    "        \"\"\"\n",
    "        Load frames from the specified video file at the given frame indices.\n",
    "        For depth videos, convert frames to single-channel grayscale.\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Error opening video file: {video_path}\")\n",
    "\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frames = []\n",
    "\n",
    "        for idx in frame_indices:\n",
    "            if idx >= total_frames:  # Ensure indices do not exceed total frames\n",
    "                break\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            if is_depth:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "                frame = np.expand_dims(frame, axis=-1)  # Add channel dimension\n",
    "\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) if not is_depth else frame\n",
    "            frames.append(F.to_tensor(frame))\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        # Handle the case where no frames were loaded\n",
    "        if not frames:\n",
    "            print(f\"No frames loaded for video: {video_path}, indices: {frame_indices}\")\n",
    "            with open(os.path.join(args.log_dir, 'training_log.txt'), 'a') as log_file:\n",
    "                log_file.write(f\"No frames loaded for video: {video_path}, indices: {frame_indices}\")\n",
    "\n",
    "            # Handle empty frames list by appending a black frame of the expected size\n",
    "            placeholder_frame = np.zeros((224, 224, 1 if is_depth else 3), dtype=np.uint8)\n",
    "            frames.append(F.to_tensor(placeholder_frame))\n",
    "\n",
    "        # Pad if fewer frames are available\n",
    "        while len(frames) < len(frame_indices):\n",
    "            frames.append(frames[-1])\n",
    "\n",
    "        return frames\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Override __getitem__ to load frames from both original and depth videos\n",
    "        with synchronized indices.\n",
    "        \"\"\"\n",
    "        # orig_video_path, depth_video_path, label = self.samples[idx]\n",
    "        orig_video_path, label = self.samples[idx]\n",
    "\n",
    "        cap = cv2.VideoCapture(orig_video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        cap.release()\n",
    "\n",
    "        # Determine frame indices\n",
    "        if self.is_train:\n",
    "            start_frame = np.random.randint(0, max(1, total_frames - self.num_frames + 1))\n",
    "        else:\n",
    "            start_frame = 0\n",
    "\n",
    "        frame_indices = np.linspace(start_frame, start_frame + self.num_frames - 1, self.num_frames, dtype=int)\n",
    "\n",
    "        # Load frames from both videos using the same frame indices\n",
    "        orig_frames = self._load_frames(orig_video_path, frame_indices, is_depth=False)\n",
    "        # depth_frames = self._load_frames(depth_video_path, frame_indices, is_depth=True)        \n",
    "        \n",
    "        # Apply transformations to both original and depth frames\n",
    "        if self.transform:\n",
    "            orig_frames = [self.transform(frame) for frame in orig_frames]\n",
    "            # depth_frames = [self.transform(frame) for frame in depth_frames]\n",
    "\n",
    "        # Apply augmentation if in training mode\n",
    "        if self.is_train:\n",
    "            angle, scale = self._random_augmentation_params()\n",
    "            orig_frames = [self.apply_augmentation(frame, angle, scale) for frame in orig_frames]\n",
    "            # depth_frames = [self.apply_augmentation(frame, angle, scale) for frame in depth_frames]\n",
    "\n",
    "        orig_frames = torch.stack(orig_frames)  # Shape: (num_frames, 3, H, W)\n",
    "        # depth_frames = torch.stack(depth_frames)  # Shape: (num_frames, 1, H, W)\n",
    "\n",
    "        # # Combine into a 4-channel tensor\n",
    "        # combined_frames = torch.cat([orig_frames, depth_frames], dim=1)  # Shape: (num_frames, 4, H, W)\n",
    "        combined_frames = orig_frames\n",
    "\n",
    "        return combined_frames, label\n",
    "        \n",
    "        \n",
    "    def _random_augmentation_params(self):\n",
    "        \"\"\"\n",
    "        Generate random augmentation parameters (angle and scale) for training.\n",
    "        \"\"\"\n",
    "        angle = random.uniform(-180, 180) if random.random() > 0.5 else 0\n",
    "        scale = random.uniform(0.7, 1.3) if random.random() > 0.5 else 1\n",
    "        return angle, scale\n",
    "\n",
    "    def apply_augmentation(self, image, angle, scale):\n",
    "        \"\"\"Apply rotation and scaling augmentation.\"\"\"\n",
    "        if angle != 0:\n",
    "            image = F.rotate(image, angle)\n",
    "        if scale != 1:\n",
    "            image = F.affine(image, angle=0, translate=(0, 0), scale=scale, shear=0)\n",
    "        return image\n",
    "\n",
    "######################################################\n",
    "\n",
    "class AdaptiveCenterCropAndResize:\n",
    "    def __init__(self, output_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            output_size (tuple or int): The desired output size after resizing (e.g., (32, 32)).\n",
    "        \"\"\"\n",
    "        self.output_size = output_size\n",
    "        self.to_pil = transforms.ToPILImage()\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Convert tensor to PIL image if necessary\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = self.to_pil(img)\n",
    "\n",
    "        # Handle single-channel images\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('L')  # Convert to grayscale mode\n",
    "            \n",
    "        # Get image size (width, height)\n",
    "        width, height = img.size\n",
    "\n",
    "        # Find the minimum dimension to create the largest possible square\n",
    "        crop_size = min(width, height)\n",
    "\n",
    "        # Calculate the coordinates to center-crop the square\n",
    "        left = (width - crop_size) // 2\n",
    "        top = (height - crop_size) // 2\n",
    "        right = (width + crop_size) // 2\n",
    "        bottom = (height + crop_size) // 2\n",
    "\n",
    "        # Crop the image to the largest square\n",
    "        img = img.crop((left, top, right, bottom))\n",
    "\n",
    "        # Resize the cropped square to the desired output size\n",
    "        img = img.resize(self.output_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "        # Convert the resized image back to a tensor\n",
    "        img = self.to_tensor(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_length = max([frames.size(0) for frames, _ in batch])  # Get the maximum sequence length\n",
    "    padded_frames = []  # To store padded 4-channel tensors\n",
    "    labels = []  # To store labels\n",
    "    \n",
    "    for frames, label in batch:\n",
    "        if frames.size(0) < max_length:\n",
    "            # Pad with zeros along the frame dimension\n",
    "            padding = torch.zeros((max_length - frames.size(0), *frames.shape[1:]))\n",
    "            # padded_frames.append(torch.cat((frames, padding), dim=0))\n",
    "            padded_frames.append(torch.cat((frames, padding), dim=0))  # Pad at the end\n",
    "        else:\n",
    "            padded_frames.append(frames)\n",
    "\n",
    "        labels.append(label)\n",
    "    \n",
    "    # Stack all sequences and labels\n",
    "    padded_frames = torch.stack(padded_frames)  # Shape: (batch_size, max_length, 4, H, W)\n",
    "    labels = torch.tensor(labels)  # Shape: (batch_size,)\n",
    "\n",
    "    return padded_frames, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41d43cb0-9ca0-476f-b999-ac4b915f7f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "############### Data Loader Hybrid (Img+Depth, 4channel) for OULU_NPU ###############\n",
    "#####################################################################################\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, orig_root_dir, depth_root_dir, transform=None, num_frames=16, is_train=False):\n",
    "        self.orig_root_dir = orig_root_dir\n",
    "        self.depth_root_dir = depth_root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = ['attack', 'real']\n",
    "        self.samples = self._load_samples()\n",
    "        self.num_frames = num_frames\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def _load_samples(self):\n",
    "        \"\"\"\n",
    "        Load paths to original videos and their corresponding depth videos, along with labels.\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        for cls in self.classes:\n",
    "            orig_cls_dir = os.path.join(self.orig_root_dir, cls)\n",
    "            for root, _, files in os.walk(orig_cls_dir):\n",
    "                for fname in files:\n",
    "                    if fname.endswith(('.mp4', '.mov', '.avi')):\n",
    "                        orig_video_path = os.path.join(root, fname)\n",
    "                                                \n",
    "                        depth_video_path = orig_video_path.replace(self.orig_root_dir, self.depth_root_dir)\n",
    "                        depth_video_path = os.path.splitext(depth_video_path)[0] + '.mp4'\n",
    "\n",
    "                        if os.path.exists(depth_video_path):\n",
    "                            samples.append((orig_video_path, depth_video_path, self.classes.index(cls)))\n",
    "\n",
    "                        else:\n",
    "                            print(f\"Warning: Depth map video not found for {orig_video_path}\")                            \n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _load_frames(self, video_path, frame_indices, is_depth=False):\n",
    "        \"\"\"\n",
    "        Load frames from the specified video file at the given frame indices.\n",
    "        For depth videos, convert frames to single-channel grayscale.\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Error opening video file: {video_path}\")\n",
    "\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frames = []\n",
    "\n",
    "        for idx in frame_indices:\n",
    "            if idx >= total_frames:  # Ensure indices do not exceed total frames\n",
    "                break\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            if is_depth:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "                frame = np.expand_dims(frame, axis=-1)  # Add channel dimension\n",
    "\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) if not is_depth else frame\n",
    "            frames.append(F.to_tensor(frame))\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        # Handle the case where no frames were loaded\n",
    "        if not frames:\n",
    "            print(f\"No frames loaded for video: {video_path}, indices: {frame_indices}\")\n",
    "            with open(os.path.join(args.log_dir, 'training_log.txt'), 'a') as log_file:\n",
    "                log_file.write(f\"No frames loaded for video: {video_path}, indices: {frame_indices}\")\n",
    "\n",
    "            # Handle empty frames list by appending a black frame of the expected size\n",
    "            placeholder_frame = np.zeros((224, 224, 1 if is_depth else 3), dtype=np.uint8)\n",
    "            frames.append(F.to_tensor(placeholder_frame))\n",
    "\n",
    "        # Pad if fewer frames are available\n",
    "        while len(frames) < len(frame_indices):\n",
    "            frames.append(frames[-1])\n",
    "\n",
    "        return frames\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Override __getitem__ to load frames from both original and depth videos\n",
    "        with synchronized indices.\n",
    "        \"\"\"\n",
    "        orig_video_path, depth_video_path, label = self.samples[idx]\n",
    "\n",
    "        cap = cv2.VideoCapture(orig_video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        cap.release()\n",
    "\n",
    "        # Determine frame indices\n",
    "        if self.is_train:\n",
    "            start_frame = np.random.randint(0, max(1, total_frames - self.num_frames + 1))\n",
    "        else:\n",
    "            start_frame = 0\n",
    "\n",
    "        frame_indices = np.linspace(start_frame, start_frame + self.num_frames - 1, self.num_frames, dtype=int)\n",
    "\n",
    "        # Load frames from both videos using the same frame indices\n",
    "        orig_frames = self._load_frames(orig_video_path, frame_indices, is_depth=False)\n",
    "        depth_frames = self._load_frames(depth_video_path, frame_indices, is_depth=True)        \n",
    "        \n",
    "        # Apply transformations to both original and depth frames\n",
    "        if self.transform:\n",
    "            orig_frames = [self.transform(frame) for frame in orig_frames]\n",
    "            depth_frames = [self.transform(frame) for frame in depth_frames]\n",
    "\n",
    "        # Apply augmentation if in training mode\n",
    "        if self.is_train:\n",
    "            angle, scale = self._random_augmentation_params()\n",
    "            orig_frames = [self.apply_augmentation(frame, angle, scale) for frame in orig_frames]\n",
    "            depth_frames = [self.apply_augmentation(frame, angle, scale) for frame in depth_frames]\n",
    "\n",
    "        orig_frames = torch.stack(orig_frames)  # Shape: (num_frames, 3, H, W)\n",
    "        depth_frames = torch.stack(depth_frames)  # Shape: (num_frames, 1, H, W)\n",
    "\n",
    "        # Combine into a 4-channel tensor\n",
    "        combined_frames = torch.cat([orig_frames, depth_frames], dim=1)  # Shape: (num_frames, 4, H, W)\n",
    "\n",
    "        return combined_frames, label\n",
    "        \n",
    "        \n",
    "    def _random_augmentation_params(self):\n",
    "        \"\"\"\n",
    "        Generate random augmentation parameters (angle and scale) for training.\n",
    "        \"\"\"\n",
    "        angle = random.uniform(-180, 180) if random.random() > 0.5 else 0\n",
    "        scale = random.uniform(0.7, 1.3) if random.random() > 0.5 else 1\n",
    "        return angle, scale\n",
    "\n",
    "    def apply_augmentation(self, image, angle, scale):\n",
    "        \"\"\"Apply rotation and scaling augmentation.\"\"\"\n",
    "        if angle != 0:\n",
    "            image = F.rotate(image, angle)\n",
    "        if scale != 1:\n",
    "            image = F.affine(image, angle=0, translate=(0, 0), scale=scale, shear=0)\n",
    "        return image\n",
    "\n",
    "######################################################\n",
    "\n",
    "class AdaptiveCenterCropAndResize:\n",
    "    def __init__(self, output_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            output_size (tuple or int): The desired output size after resizing (e.g., (32, 32)).\n",
    "        \"\"\"\n",
    "        self.output_size = output_size\n",
    "        self.to_pil = transforms.ToPILImage()\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Convert tensor to PIL image if necessary\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = self.to_pil(img)\n",
    "\n",
    "        # Handle single-channel images\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('L')  # Convert to grayscale mode\n",
    "            \n",
    "        # Get image size (width, height)\n",
    "        width, height = img.size\n",
    "\n",
    "        # Find the minimum dimension to create the largest possible square\n",
    "        crop_size = min(width, height)\n",
    "\n",
    "        # Calculate the coordinates to center-crop the square\n",
    "        left = (width - crop_size) // 2\n",
    "        top = (height - crop_size) // 2\n",
    "        right = (width + crop_size) // 2\n",
    "        bottom = (height + crop_size) // 2\n",
    "\n",
    "        # Crop the image to the largest square\n",
    "        img = img.crop((left, top, right, bottom))\n",
    "\n",
    "        # Resize the cropped square to the desired output size\n",
    "        img = img.resize(self.output_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "        # Convert the resized image back to a tensor\n",
    "        img = self.to_tensor(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_length = max([frames.size(0) for frames, _ in batch])  # Get the maximum sequence length\n",
    "    padded_frames = []  # To store padded 4-channel tensors\n",
    "    labels = []  # To store labels\n",
    "    \n",
    "    for frames, label in batch:\n",
    "        if frames.size(0) < max_length:\n",
    "            # Pad with zeros along the frame dimension\n",
    "            padding = torch.zeros((max_length - frames.size(0), *frames.shape[1:]))\n",
    "            # padded_frames.append(torch.cat((frames, padding), dim=0))\n",
    "            padded_frames.append(torch.cat((frames, padding), dim=0))  # Pad at the end\n",
    "        else:\n",
    "            padded_frames.append(frames)\n",
    "\n",
    "        labels.append(label)\n",
    "    \n",
    "    # Stack all sequences and labels\n",
    "    padded_frames = torch.stack(padded_frames)  # Shape: (batch_size, max_length, 4, H, W)\n",
    "    labels = torch.tensor(labels)  # Shape: (batch_size,)\n",
    "\n",
    "    return padded_frames, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d9a8bf0-6eb7-48a0-9bed-d4e0c3b0eaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "############### Models #########################\n",
    "################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ebc04fe-8c6e-45a4-909e-e9e013876996",
   "metadata": {},
   "outputs": [],
   "source": [
    "### model_mobilenetv3l ###\n",
    "\n",
    "class CNNTemporalAvgPoolingMBNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(CNNTemporalAvgPoolingMBNet, self).__init__()\n",
    "\n",
    "        # Use MobileNetV3 as the CNN backbone\n",
    "        # self.cnn = models.mobilenet_v3_large(pretrained=True)\n",
    "        self.cnn = models.mobilenet_v3_large()\n",
    "        self.cnn.classifier = nn.Identity()  # Remove the classifier to use the feature extractor\n",
    "        # Adjust the final fully connected layer to match MobileNetV3's feature size (960 instead of 1280)\n",
    "        self.fc = nn.Linear(960, num_classes)\n",
    "        \n",
    "        # # Use ResNet-101 as the CNN backbone\n",
    "        # # self.cnn = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
    "        # self.cnn = models.resnet101()\n",
    "        # self.cnn.fc = nn.Identity()  # Remove the classifier to use the feature extractor\n",
    "        # self.fc = nn.Linear(2048, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, C, H, W = x.size()  # Expect input as [batch_size, seq_len, channels, height, width]\n",
    "        out_decision_t = []\n",
    "        cnn_features = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # with torch.no_grad():\n",
    "            feature = self.cnn(x[:, t, :, :, :])  # Extract CNN features for each frame\n",
    "            cnn_features.append(feature)\n",
    "            # out_decision_t.append(self.fc(feature))\n",
    "        \n",
    "        # Stack features from all frames and average over the temporal dimension (seq_len)\n",
    "        # out_decision_t = torch.stack(out_decision_t, dim=1)  # Shape: [batch_size, seq_len, 960]\n",
    "        # out = out_decision_t.mean(dim=1)  # Temporal average pooling: Shape: [batch_size, 960]\n",
    "\n",
    "        cnn_features = torch.stack(cnn_features, dim=1)  # Shape: [batch_size, seq_len, 960]\n",
    "        temporal_avg_features = cnn_features.mean(dim=1)  # Temporal average pooling: Shape: [batch_size, 960]\n",
    "\n",
    "        # Pass through the final fully connected layer\n",
    "        out = self.fc(temporal_avg_features)  # Shape: [batch_size, num_classes]\n",
    "        return out\n",
    "\n",
    "    def extract_intermediate_features(self, x):\n",
    "        \"\"\"Extract features before and after LSTM.\"\"\"\n",
    "        batch_size, seq_len, C, H, W = x.size()\n",
    "        cnn_features = []\n",
    "        for t in range(seq_len):\n",
    "            # with torch.no_grad():\n",
    "            feature = self.cnn(x[:, t, :, :, :])  # CNN output (before LSTM)\n",
    "            cnn_features.append(feature)\n",
    "        cnn_features = torch.stack(cnn_features, dim=1)  # Shape: [batch_size, seq_len, cnn_output_dim]\n",
    "        temporal_avg_features = cnn_features.mean(dim=1)  # Temporal average pooling: Shape: [batch_size, 960]\n",
    "        return cnn_features, temporal_avg_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1255e71c-e02f-489a-b2dd-716f91a91d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "### model_resnet101 ###\n",
    "\n",
    "class CNNTemporalAvgPoolingR101(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(CNNTemporalAvgPoolingR101, self).__init__()\n",
    "\n",
    "        # # Use MobileNetV3 as the CNN backbone\n",
    "        # self.cnn = models.mobilenet_v3_large(pretrained=True)\n",
    "        # self.cnn.classifier = nn.Identity()  # Remove the classifier to use the feature extractor\n",
    "        # # Adjust the final fully connected layer to match MobileNetV3's feature size (960 instead of 1280)\n",
    "        # self.fc = nn.Linear(960, num_classes)\n",
    "        \n",
    "        # Use ResNet-101 as the CNN backbone\n",
    "        # self.cnn = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
    "        self.cnn = models.resnet101()\n",
    "        self.cnn.fc = nn.Identity()  # Remove the classifier to use the feature extractor\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, C, H, W = x.size()  # Expect input as [batch_size, seq_len, channels, height, width]\n",
    "        out_decision_t = []\n",
    "        cnn_features = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # with torch.no_grad():\n",
    "            feature = self.cnn(x[:, t, :, :, :])  # Extract CNN features for each frame\n",
    "            cnn_features.append(feature)\n",
    "            # out_decision_t.append(self.fc(feature))\n",
    "        \n",
    "        # Stack features from all frames and average over the temporal dimension (seq_len)\n",
    "        # out_decision_t = torch.stack(out_decision_t, dim=1)  # Shape: [batch_size, seq_len, 960]\n",
    "        # out = out_decision_t.mean(dim=1)  # Temporal average pooling: Shape: [batch_size, 960]\n",
    "\n",
    "        cnn_features = torch.stack(cnn_features, dim=1)  # Shape: [batch_size, seq_len, 960]\n",
    "        temporal_avg_features = cnn_features.mean(dim=1)  # Temporal average pooling: Shape: [batch_size, 960]\n",
    "\n",
    "        # Pass through the final fully connected layer\n",
    "        out = self.fc(temporal_avg_features)  # Shape: [batch_size, num_classes]\n",
    "        return out\n",
    "\n",
    "    def extract_intermediate_features(self, x):\n",
    "        \"\"\"Extract features before and after LSTM.\"\"\"\n",
    "        batch_size, seq_len, C, H, W = x.size()\n",
    "        cnn_features = []\n",
    "        for t in range(seq_len):\n",
    "            # with torch.no_grad():\n",
    "            feature = self.cnn(x[:, t, :, :, :])  # CNN output (before LSTM)\n",
    "            cnn_features.append(feature)\n",
    "        cnn_features = torch.stack(cnn_features, dim=1)  # Shape: [batch_size, seq_len, cnn_output_dim]\n",
    "        temporal_avg_features = cnn_features.mean(dim=1)  # Temporal average pooling: Shape: [batch_size, 960]\n",
    "        return cnn_features, temporal_avg_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de411546-daac-487b-80cd-fc3b8b428a51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7c135c6-b6fc-4634-9e21-e9143a23b6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Hybrid Models ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "258ea1b2-7f26-45db-90df-fddf04b17efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Multi-Channel Input with Channel Expansion\n",
    "# Input: 224*224*4, image + depth_map\n",
    "# Modified first Conv2d layer to take 4 input channels\n",
    "\n",
    "class MultiChannelCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(MultiChannelCNN, self).__init__()\n",
    "        \n",
    "        # Load MobileNetV3 as the CNN backbone\n",
    "        mobilenet_v3 = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.DEFAULT)\n",
    "        \n",
    "        # Modify the first convolutional layer to accept 4 input channels (RGB + Depth)\n",
    "        original_conv1 = mobilenet_v3.features[0][0]  # Access the first Conv2d layer\n",
    "        self.custom_conv1 = nn.Conv2d(\n",
    "            in_channels=4,  # Change input channels to 4\n",
    "            out_channels=original_conv1.out_channels,\n",
    "            kernel_size=original_conv1.kernel_size,\n",
    "            stride=original_conv1.stride,\n",
    "            padding=original_conv1.padding,\n",
    "            bias=original_conv1.bias\n",
    "        )\n",
    "        \n",
    "        # Copy the weights of the original 3-channel convolution to the first 3 channels\n",
    "        with torch.no_grad():\n",
    "            self.custom_conv1.weight[:, :3, :, :] = original_conv1.weight\n",
    "            self.custom_conv1.weight[:, 3:, :, :] = original_conv1.weight.mean(dim=1, keepdim=True)\n",
    "        \n",
    "        # Replace the original first layer in MobileNetV3\n",
    "        mobilenet_v3.features[0][0] = self.custom_conv1\n",
    "        \n",
    "        # Remove the classifier to use MobileNetV3 as a feature extractor\n",
    "        self.cnn = mobilenet_v3\n",
    "        self.cnn.classifier = nn.Identity()  # No classification head\n",
    "        \n",
    "        # Fully connected layer for the final classification\n",
    "        self.fc = nn.Linear(960, num_classes)  # 960 is the output feature size of MobileNetV3\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for multi-channel input.\n",
    "        x_image: Tensor of shape [batch_size, seq_len, 3, 224, 224] (RGB frames)\n",
    "        x_depth: Tensor of shape [batch_size, seq_len, 1, 224, 224] (Depth frames)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _, H, W = x.size()\n",
    "        \n",
    "        # # Concatenate RGB and Depth channels along the channel dimension\n",
    "        # x = torch.cat([x_image, x_depth], dim=2)  # Shape: [batch_size, seq_len, 4, 224, 224]\n",
    "        \n",
    "        cnn_features = []\n",
    "        for t in range(seq_len):\n",
    "            feature = self.cnn(x[:, t, :, :, :])  # Extract features for each frame\n",
    "            cnn_features.append(feature)\n",
    "        \n",
    "        # Temporal average pooling\n",
    "        cnn_features = torch.stack(cnn_features, dim=1)  # Shape: [batch_size, seq_len, 960]\n",
    "        temporal_avg_features = cnn_features.mean(dim=1)  # Shape: [batch_size, 960]\n",
    "\n",
    "        # Pass through the final fully connected layer\n",
    "        out = self.fc(temporal_avg_features)  # Shape: [batch_size, num_classes]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5abeb01-cea8-4999-826b-92e0647de12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Separate Branches with Feature Concatenation\n",
    "# Input: 224*224*4, image + depth_map\n",
    "# RGB branch MobileNet with 3-channel input\n",
    "# Depth map branch MobileNet with 1-channel input\n",
    "# Output features from both branches are concatenated, and passed to classifier\n",
    "\n",
    "class DualBranchMobileNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(DualBranchMobileNet, self).__init__()\n",
    "        \n",
    "        # RGB branch: Pretrained MobileNetV3 for RGB input (3 channels)\n",
    "        self.rgb_branch = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.DEFAULT)\n",
    "        # Modify classifier for the RGB branch\n",
    "        rgb_features_in = self.rgb_branch.classifier[0].in_features\n",
    "        self.rgb_branch.classifier = nn.Identity()  # Remove classifier, keep feature extractor\n",
    "        \n",
    "        # Depth branch: Another MobileNetV3 for depth map (1 channel)\n",
    "        self.depth_branch = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.DEFAULT)\n",
    "        # Modify the first layer of the depth branch to accept 1-channel input\n",
    "        self.depth_branch.features[0][0] = nn.Conv2d(\n",
    "            1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
    "        )\n",
    "        self.depth_branch.classifier = nn.Identity()  # Remove classifier for feature extraction\n",
    "\n",
    "        # Concatenate the output of both branches and pass to the classifier\n",
    "        concat_feature_size = rgb_features_in * 2  # Combine features from both branches\n",
    "        \n",
    "        # Final classifier (after feature concatenation)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(concat_feature_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: [batch_size, num_frames=1, channels=4, height=224, width=224]\n",
    "        batch_size, num_frames, channels, height, width = x.shape\n",
    "        assert num_frames == 1, \"This model supports single-frame input only.\"\n",
    "        \n",
    "        # Squeeze the frame dimension\n",
    "        x = x.squeeze(1)  # Shape: [batch_size, channels=4, height, width]\n",
    "        \n",
    "        # Split the input into RGB (3 channels) and depth (1 channel)\n",
    "        rgb_input = x[:, :3, :, :]  # First 3 channels for RGB\n",
    "        depth_input = x[:, 3:, :, :]  # Last channel for depth map\n",
    "        \n",
    "        # Pass through the RGB branch\n",
    "        rgb_features = self.rgb_branch(rgb_input)  # Shape: [batch_size, rgb_features_in]\n",
    "        \n",
    "        # Pass through the depth branch\n",
    "        depth_features = self.depth_branch(depth_input)  # Shape: [batch_size, rgb_features_in]\n",
    "        \n",
    "        # Concatenate the features from both branches\n",
    "        combined_features = torch.cat((rgb_features, depth_features), dim=1)  # Shape: [batch_size, concat_feature_size]\n",
    "        \n",
    "        # Pass through the final classifier\n",
    "        output = self.classifier(combined_features)  # Shape: [batch_size, num_classes]\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62996585-6af1-4ab5-be44-742fb5d37976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Depth as Auxiliary Input\n",
    "# Input: 224*224*4, image + depth_map\n",
    "# MobileNet with 3-channel RGB input\n",
    "# Simple CNN for depth features\n",
    "# RGB features before MobileNet final classification layer are modulated, by depth features\n",
    "\n",
    "class DepthAuxiliaryMobileNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(DepthAuxiliaryMobileNet, self).__init__()\n",
    "        \n",
    "        # RGB branch: Pretrained MobileNetV3 for RGB input (3 channels)\n",
    "        self.rgb_branch = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.DEFAULT)\n",
    "        rgb_features_in = self.rgb_branch.classifier[0].in_features\n",
    "        self.rgb_branch.classifier = nn.Identity()  # Remove the classification head\n",
    "        \n",
    "        # Depth branch: Simple CNN for extracting depth features\n",
    "        self.depth_branch = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.depth_feature_dim = 32  # Output size from the depth branch\n",
    "        \n",
    "        # Depth modulation layer to integrate depth features with RGB features\n",
    "        self.modulation_layer = nn.Sequential(\n",
    "            nn.Linear(self.depth_feature_dim, rgb_features_in),\n",
    "            nn.Sigmoid()  # Output range [0, 1] for modulation\n",
    "        )\n",
    "        \n",
    "        # Final classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rgb_features_in, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: [batch_size, num_frames=1, channels=4, height=224, width=224]\n",
    "        batch_size, num_frames, channels, height, width = x.shape\n",
    "        assert num_frames == 1, \"This model supports single-frame input only.\"\n",
    "        \n",
    "        # Squeeze the frame dimension\n",
    "        x = x.squeeze(1)  # Shape: [batch_size, channels=4, height, width]\n",
    "        \n",
    "        # Split the input into RGB (3 channels) and depth (1 channel)\n",
    "        rgb_input = x[:, :3, :, :]  # First 3 channels for RGB\n",
    "        depth_input = x[:, 3:, :, :]  # Last channel for depth map\n",
    "        \n",
    "        # Pass through the RGB branch\n",
    "        rgb_features = self.rgb_branch(rgb_input)  # Shape: [batch_size, rgb_features_in]\n",
    "        \n",
    "        # Pass through the depth branch\n",
    "        depth_features = self.depth_branch(depth_input)  # Shape: [batch_size, 32, 1, 1]\n",
    "        depth_features = depth_features.view(batch_size, -1)  # Flatten: [batch_size, 32]\n",
    "        \n",
    "        # Modulate RGB features with depth features\n",
    "        modulation_weights = self.modulation_layer(depth_features)  # Shape: [batch_size, rgb_features_in]\n",
    "        modulated_rgb_features = rgb_features * modulation_weights  # Element-wise multiplication\n",
    "        \n",
    "        # Pass through the final classifier\n",
    "        output = self.classifier(modulated_rgb_features)  # Shape: [batch_size, num_classes]\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26dffb3f-b987-41aa-87df-d20028a5d621",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "############### Eval utils #####################\n",
    "################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "420ef067-b699-4beb-bebe-69bf0b1d5356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    all_times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Start timing for prediction\n",
    "            start_time = time.time()\n",
    "            outputs = model(inputs)\n",
    "            inference_time = time.time() - start_time\n",
    "            all_times.append(inference_time)\n",
    "            \n",
    "            # Loss calculation\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # For AUC, EER, etc.\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1]  # Assuming class 1 is the target class\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Accuracy calculation\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate metrics like AUC-ROC, EER, etc. based on collected labels and probabilities\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "    auc_roc = auc(fpr, tpr)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = thresholds[np.nanargmin(np.absolute(fnr - fpr))]\n",
    "    eer = fpr[np.nanargmin(np.absolute(fnr - fpr))]\n",
    "\n",
    "    # Calculate FAR, FRR, HTER, and Youden's Index\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    youdens_index = tpr[optimal_idx] - fpr[optimal_idx]\n",
    "    far = fpr[optimal_idx]\n",
    "    frr = fnr[optimal_idx]\n",
    "    hter = (far + frr) / 2\n",
    "\n",
    "    # Average inference time\n",
    "    avg_inference_time = np.mean(all_times)\n",
    "\n",
    "    # Test loss and accuracy\n",
    "    test_loss = running_loss / len(loader.dataset)\n",
    "    test_acc = 100. * correct / total\n",
    "\n",
    "    # Return dictionary with all results\n",
    "    return {\n",
    "        'test_loss': test_loss,\n",
    "        'test_acc': test_acc,\n",
    "        'auc_roc': auc_roc,\n",
    "        'eer': eer,\n",
    "        'hter': hter,\n",
    "        'far': far,\n",
    "        'frr': frr,\n",
    "        'youdens_index': youdens_index,\n",
    "        'optimal_threshold': optimal_threshold,\n",
    "        'avg_inference_time': avg_inference_time,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'labels': all_labels,\n",
    "        'probs': all_probs\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32994148-838a-4b7d-8be1-76374e6d00e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "\n",
    "def generate_evaluation_summary(results):\n",
    "    # Extract metrics from the results dictionary\n",
    "    test_loss = results['test_loss']\n",
    "    test_acc = results['test_acc']\n",
    "    auc_roc = results['auc_roc']\n",
    "    eer = results['eer']\n",
    "    hter = results['hter']\n",
    "    far = results['far']\n",
    "    frr = results['frr']\n",
    "    youdens_index = results['youdens_index']\n",
    "    optimal_threshold = results['optimal_threshold']\n",
    "    avg_inference_time = results['avg_inference_time']\n",
    "\n",
    "#     # Print summary\n",
    "#     print(\"\\n--- Evaluation Summary ---\")\n",
    "#     print(f\"Test Loss: {test_loss:.4f}\")\n",
    "#     print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "#     print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "#     print(f\"Equal Error Rate (EER): {eer:.4f}\")\n",
    "#     print(f\"Half Total Error Rate (HTER): {hter:.4f}\")\n",
    "#     print(f\"False Acceptance Rate (FAR): {far:.4f}\")\n",
    "#     print(f\"False Rejection Rate (FRR): {frr:.4f}\")\n",
    "#     print(f\"Youden's Index (Max): {youdens_index:.4f}\")\n",
    "#     print(f\"Optimal Threshold (Youden's Index): {optimal_threshold:.4f}\")\n",
    "#     print(f\"Average inference time per sample: {avg_inference_time:.6f} seconds\")\n",
    "\n",
    "#     # Plot AUC-ROC Curve\n",
    "#     plot_roc_curve(results)\n",
    "\n",
    "    print(\"\\n--- Evaluation Summary ---\")\n",
    "    print(f\"HTER (%), AUC-ROC, Test Accuracy (%)\")\n",
    "    print(f\"{hter*100:.4f}, {auc_roc:.4f}, {test_acc:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    return hter, auc_roc, test_acc\n",
    "\n",
    "def plot_roc_curve(results):\n",
    "    \"\"\"Generate ROC curve from the evaluation results.\"\"\"\n",
    "    fpr = results['fpr']\n",
    "    tpr = results['tpr']\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='blue', label=f'AUC = {results[\"auc_roc\"]:.4f}')\n",
    "    plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_error_histogram(far, frr, eer):\n",
    "    \"\"\"Plot histograms of FAR, FRR, and EER values.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot FAR\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.bar(['FAR'], [far], color='red')\n",
    "    plt.ylabel('Rate')\n",
    "    plt.title('False Acceptance Rate (FAR)')\n",
    "\n",
    "    # Plot FRR\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.bar(['FRR'], [frr], color='blue')\n",
    "    plt.ylabel('Rate')\n",
    "    plt.title('False Rejection Rate (FRR)')\n",
    "\n",
    "    # Plot EER\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.bar(['EER'], [eer], color='green')\n",
    "    plt.ylabel('Rate')\n",
    "    plt.title('Equal Error Rate (EER)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_inference_time(avg_inference_time):\n",
    "    \"\"\"Plot inference time as a bar chart.\"\"\"\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(['Average Inference Time'], [avg_inference_time], color='purple')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.title('Average Inference Time per Sample')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55cc79e1-67c4-4ba5-9686-5ead68e835c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "############### Eval results ###################\n",
    "################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04b70bf4-af29-41aa-ae49-9d57f1434ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Eval results Img Only Models ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f3bc7e4-b309-47bc-bd41-cf977071cb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and setup\n",
    "criterion = nn.CrossEntropyLoss()  # For final classification\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "device = torch.device('cuda:1')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    AdaptiveCenterCropAndResize((224, 224)),  # Adaptive crop, resize, and convert to tensor\n",
    "    # transforms.ToPILImage(),\n",
    "    # transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "###################################################\n",
    "img_dataset_path = '/home/muhammad_jabbar/face_PAD/datasets/Rose_Youtu'\n",
    "img_datasets = ['/test']\n",
    "\n",
    "depth_dataset_path = '/home/muhammad_jabbar/face_PAD/datasets/Rose_Youtu_depth_mp4/DepthAnythingV2_Base'\n",
    "depth_datasets = ['/test']\n",
    "###################################################\n",
    "\n",
    "# mobilenet_v3_large checkpoints\n",
    "checkpoints_m = [\n",
    "    # '/home/muhammad_jabbar/face_PAD/datasets/Replay_Attack_mp4/mobilenet_v3_large/checkpoints_1frame/checkpoint_epoch_25.pth',\n",
    "    # '/home/muhammad_jabbar/face_PAD/datasets/Replay-Mobile/mobilenet_v3_large/checkpoints_1frame/checkpoint_epoch_32.pth',\n",
    "    # '/home/muhammad_jabbar/face_PAD/datasets/Rose_Youtu/mobilenet_v3_large/checkpoints_1frame/checkpoint_epoch_22.pth',\n",
    "    # '/home/muhammad_jabbar/face_PAD/logs/log_014_20241216_155805/checkpoints/best_model.pth',\n",
    "\n",
    "    # Image Models (Mobilenet_v3_large, no pretrained weights)\n",
    "    # '/home/muhammad_jabbar/face_PAD/RY_20250227/CNN_FacePAD_RY_MobileNetv3Large_NoWeights_train/logs/log_001_20250227_143008/checkpoints/best_model.pth', # Depth, MobileNetV3Large (No pretrained weights)\n",
    "\n",
    "    # # Depth Models (Mobilenet_v3_large, no pretrained weights)\n",
    "    '/home/muhammad_jabbar/face_PAD/RY_20250227/CNN_FacePAD_RY_MobileNetv3Large_NoWeights_train/logs/log_002_20250227_143134/checkpoints/best_model.pth', # Depth, MobileNetV3Large (No pretrained weights)\n",
    "]\n",
    "\n",
    "# resnet101 checkpoints\n",
    "checkpoints_r = [\n",
    "    # '/home/muhammad_jabbar/face_PAD/datasets/Replay_Attack_mp4/resnet101/checkpoints_1frame/checkpoint_epoch_9.pth',\n",
    "    # '/home/muhammad_jabbar/face_PAD/datasets/Replay-Mobile/resnet101/checkpoints_1frame/checkpoint_epoch_9.pth',\n",
    "    # '/home/muhammad_jabbar/face_PAD/datasets/Rose_Youtu/resnet101/checkpoints_1frame/checkpoint_epoch_20.pth'\n",
    "]\n",
    "\n",
    "# chkpt_paths = [checkpoints_m, checkpoints_r]\n",
    "chkpt_paths = [checkpoints_m]\n",
    "\n",
    "###################################################\n",
    "\n",
    "# Instantiate the model\n",
    "model_mobilenetv3l = CNNTemporalAvgPoolingMBNet(num_classes=2).to(device)\n",
    "# model_resnet101 = CNNTemporalAvgPoolingR101(num_classes=2).to(device)\n",
    "\n",
    "# modelz = [model_mobilenetv3l, model_resnet101]\n",
    "# model_names = ['model_mobilenetv3l', 'model_resnet101']\n",
    "\n",
    "modelz = [model_mobilenetv3l]\n",
    "model_names = ['model_mobilenetv3l']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "061ea7ac-e94c-4ee2-9d82-dbe7c14d7ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "\n",
      "Model: model_mobilenetv3l\n",
      "Trained model checkpoint path : /home/muhammad_jabbar/face_PAD/RY_20250227/CNN_FacePAD_RY_MobileNetv3Large_NoWeights_train/logs/log_002_20250227_143134/checkpoints/best_model.pth\n",
      "Test img dataet path # : /home/muhammad_jabbar/face_PAD/datasets/Rose_Youtu_depth_mp4/DepthAnythingV2_Base/test\n",
      "Test samples: 1748\n",
      "Test dataet batches # : 7\n",
      "\n",
      "\n",
      "--- Evaluation Summary ---\n",
      "HTER (%), AUC-ROC, Test Accuracy (%)\n",
      "16.7691, 0.9067, 85.9268\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "hters = []\n",
    "auc_rocs = []\n",
    "test_accs = []\n",
    "\n",
    "num_frames = 1\n",
    "batch_size=256\n",
    "\n",
    "n_split = 1\n",
    "\n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "for i, model in enumerate(modelz):\n",
    "    for j, chkpt_path in enumerate(chkpt_paths[i]):\n",
    "        for k, img_dataset in enumerate(img_datasets):\n",
    "\n",
    "            test_dataset = VideoDataset3ch(\n",
    "                # orig_root_dir = img_dataset_path + img_dataset,\n",
    "                orig_root_dir = depth_dataset_path + img_dataset,\n",
    "                # depth_root_dir = depth_dataset_path + depth_datasets[k],\n",
    "                transform=transform,\n",
    "                num_frames=num_frames,\n",
    "                is_train=False,\n",
    "            )\n",
    "\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "            checkpoint = torch.load(chkpt_path, weights_only=True)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "            print()            \n",
    "            print(f'Model: {model_names[i]}')\n",
    "            print(f'Trained model checkpoint path : {chkpt_path}')\n",
    "            # print(f'Test img dataet path # : {img_dataset_path + img_dataset}')\n",
    "            print(f'Test img dataet path # : {depth_dataset_path + img_dataset}')\n",
    "            print(f\"Test samples: {len(test_dataset)}\")\n",
    "            print(f'Test dataet batches # : {len(test_loader)}')\n",
    "            print()\n",
    "            \n",
    "            # Assuming `evaluate` returns the results dictionary\n",
    "            results = evaluate_all(model, test_loader, criterion)\n",
    "\n",
    "            # Generate the summary and plot graphs\n",
    "            hter, auc_roc, test_acc = generate_evaluation_summary(results)\n",
    "            hters.append(hter)\n",
    "            auc_rocs.append(auc_roc)\n",
    "            test_accs.append(test_acc)\n",
    "            \n",
    "            print()\n",
    "            print('-----------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df882ef1-3323-4e36-b40a-c996edcbd647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "\n",
      "Model: model_mobilenetv3l\n",
      "Trained model checkpoint path : /home/muhammad_jabbar/face_PAD/logs/ouluNPU_ProtocolWise/log_009_20250202_090718/checkpoints/best_model.pth\n",
      "Test img dataet path # : /data/muhammad_jabbar/datasets/Oulu_NPU/Test_files\n",
      "Test depth dataet path # : /data/muhammad_jabbar/datasets/Oulu_NPU_depth_mp4/Test_files\n",
      "OULU-NPU Protocol File List # : /data/muhammad_jabbar/datasets/Oulu_NPU/Baseline/Protocol_1/Test.txt\n",
      "OULU-NPU Protocol # : 1\n",
      "Test samples: 600\n",
      "Test dataet batches # : 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:01<00:00, 20.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APCER, BPCER, ACER\n",
      "32.08333333, 30.00000000, 31.04166667\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "Model: model_mobilenetv3l\n",
      "Trained model checkpoint path : /home/muhammad_jabbar/face_PAD/logs/ouluNPU_ProtocolWise/log_010_20250202_090746/checkpoints/best_model.pth\n",
      "Test img dataet path # : /data/muhammad_jabbar/datasets/Oulu_NPU/Test_files\n",
      "Test depth dataet path # : /data/muhammad_jabbar/datasets/Oulu_NPU_depth_mp4/Test_files\n",
      "OULU-NPU Protocol File List # : /data/muhammad_jabbar/datasets/Oulu_NPU/Baseline/Protocol_2/Test.txt\n",
      "OULU-NPU Protocol # : 2\n",
      "Test samples: 1080\n",
      "Test dataet batches # : 5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:49<00:00, 21.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APCER, BPCER, ACER\n",
      "15.00000000, 12.77777778, 13.88888889\n",
      "\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### OULU APCER, BPCER, and ACER Calculation ###\n",
    "import oulumetrics\n",
    "\n",
    "num_frames = 1\n",
    "batch_size=256\n",
    "\n",
    "n_split = 1\n",
    "\n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "for i, model in enumerate(modelz):\n",
    "    for j, chkpt_path in enumerate(chkpt_paths[i]):\n",
    "\n",
    "        # protocol = 'all' if j==0 else str(j)\n",
    "        protocol = str(j+1)\n",
    "\n",
    "        if protocol=='3' or protocol=='4':\n",
    "            protocol_flist = f'/data/muhammad_jabbar/datasets/Oulu_NPU/Baseline/Protocol_{protocol}/Test_{n_split}.txt'\n",
    "        else:\n",
    "            protocol_flist = f'/data/muhammad_jabbar/datasets/Oulu_NPU/Baseline/Protocol_{protocol}/Test.txt'\n",
    "\n",
    "        for k, img_dataset in enumerate(img_datasets):\n",
    "            \n",
    "            test_dataset = VideoDataset3ch(\n",
    "                # orig_root_dir = img_dataset_path + img_dataset,\n",
    "                orig_root_dir = depth_dataset_path + img_dataset,\n",
    "                # depth_root_dir = depth_dataset_path + depth_datasets[k],\n",
    "                file_list_path = protocol_flist,\n",
    "                transform=transform,\n",
    "                num_frames=num_frames,\n",
    "                is_train=False,\n",
    "                protocol = protocol\n",
    "            )\n",
    "\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "            checkpoint = torch.load(chkpt_path, weights_only=True)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "            print()            \n",
    "            print(f'Model: {model_names[i]}')\n",
    "            print(f'Trained model checkpoint path : {chkpt_path}')\n",
    "            print(f'Test img dataet path # : {img_dataset_path + img_dataset}')\n",
    "            print(f'Test depth dataet path # : {depth_dataset_path + depth_datasets[k]}')\n",
    "            print(f'OULU-NPU Protocol File List # : {protocol_flist}')\n",
    "            print(f'OULU-NPU Protocol # : {protocol}')\n",
    "            print(f\"Test samples: {len(test_dataset)}\")\n",
    "            print(f'Test dataet batches # : {len(test_loader)}')\n",
    "            print()\n",
    "            \n",
    "            y_attack_types = []\n",
    "            y_pred = []\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in tqdm(test_loader):\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                    # Perform inference\n",
    "                    outputs = model(inputs)\n",
    "                    probabilities = torch.softmax(outputs, dim=1)  # Assuming outputs are logits\n",
    "                    predictions = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "                    # Append ground truth and predictions to lists\n",
    "                    y_attack_types.extend(labels.cpu().numpy())\n",
    "                    y_pred.extend(predictions.cpu().numpy())\n",
    "\n",
    "            # Convert lists to numpy arrays\n",
    "            y_attack_types = np.array(y_attack_types)\n",
    "            y_pred = np.array(y_pred)\n",
    "\n",
    "            apcer, bpcer, acer = oulumetrics.calculate_metrics(y_attack_types, y_pred)\n",
    "\n",
    "            # Print the results\n",
    "            # print(f\"APCER: {apcer*100:.8f}, BPCER: {bpcer*100:.8f}, ACER: {acer*100:.8f}\")                    \n",
    "            print(f\"APCER, BPCER, ACER\")                    \n",
    "            print(f\"{apcer*100:.8f}, {bpcer*100:.8f}, {acer*100:.8f}\")                    \n",
    "            \n",
    "            print()\n",
    "            print('-----------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb325f39-c3e0-423e-b2d4-1d0debfd701d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ca63a4f-880a-4944-b0b9-977ce82fdb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model checkpoint path : /home/muhammad_jabbar/face_PAD/logs/ouluNPU_ProtocolWise/log_010_20250202_090746/checkpoints/best_model.pth\n",
      "Test dataet path # : /data/muhammad_jabbar/datasets/Oulu_NPU/Test_files\n",
      "Test samples: 1080\n",
      "OULU-NPU protocol # : 2\n",
      "OULU-NPU protocol file list # : /data/muhammad_jabbar/datasets/Oulu_NPU/Baseline/Protocol_2/Test.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:47<00:00, 21.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APCER: 0.00000000, BPCER: 12.77777778, ACER: 6.38888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### OULU APCER, BPCER, and ACER Calculation ###\n",
    "import oulumetrics\n",
    "\n",
    "protocol = '2'\n",
    "n_split = 1\n",
    "\n",
    "num_frames = 1\n",
    "batch_size=256\n",
    "\n",
    "model = modelz[0]\n",
    "# chkpt_path = '/home/muhammad_jabbar/face_PAD/logs/log_014_20241216_155805/checkpoints/best_model.pth'\n",
    "# chkpt_path = '/home/muhammad_jabbar/face_PAD/logs/ouluNPU_ProtocolWise/log_001_20250129_160959/checkpoints/best_model.pth' # Img, protocol 1\n",
    "# chkpt_path = '/home/muhammad_jabbar/face_PAD/logs/ouluNPU_ProtocolWise/log_002_20250129_161355/checkpoints/best_model.pth' # Img, protocol 2\n",
    "# chkpt_path = '/home/muhammad_jabbar/face_PAD/logs/ouluNPU_ProtocolWise/log_009_20250202_090718/checkpoints/best_model.pth' # Depth, protocol 1\n",
    "# chkpt_path = '/home/muhammad_jabbar/face_PAD/logs/ouluNPU_ProtocolWise/log_010_20250202_090746/checkpoints/best_model.pth' # Depth, protocol 2\n",
    "\n",
    "\n",
    "# chkpt_path = '/home/muhammad_jabbar/face_PAD/logs/ouluNPU_ProtocolWise/log_010_20250202_090746/checkpoints/best_model.pth' # Depth, protocol 2\n",
    "\n",
    "if protocol=='3' or protocol=='4':\n",
    "    protocol_flist = f'/data/muhammad_jabbar/datasets/Oulu_NPU/Baseline/Protocol_{protocol}/Test_{n_split}.txt'\n",
    "else:\n",
    "    protocol_flist = f'/data/muhammad_jabbar/datasets/Oulu_NPU/Baseline/Protocol_{protocol}/Test.txt'\n",
    "\n",
    "test_dataset = VideoDataset3ch(\n",
    "    orig_root_dir = img_dataset_path + img_datasets[0],\n",
    "    # orig_root_dir = depth_dataset_path + img_datasets[0],\n",
    "    file_list_path = protocol_flist,\n",
    "    transform=transform,\n",
    "    num_frames=num_frames,\n",
    "    is_train=False,\n",
    "    protocol = protocol\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "checkpoint = torch.load(chkpt_path, weights_only=True)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "y_attack_types = []\n",
    "y_pred = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(f'Trained model checkpoint path : {chkpt_path}')\n",
    "print(f'Test dataet path # : {img_dataset_path + img_datasets[0]}')\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f'OULU-NPU protocol # : {protocol}')\n",
    "print(f'OULU-NPU protocol file list # : {protocol_flist}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Perform inference\n",
    "        outputs = model(inputs)\n",
    "        probabilities = torch.softmax(outputs, dim=1)  \n",
    "        predictions = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "        # Append ground truth and predictions to lists\n",
    "        y_attack_types.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "y_attack_types = np.array(y_attack_types)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "apcer, bpcer, acer = oulumetrics.calculate_metrics(y_attack_types, y_pred)\n",
    "\n",
    "# Print the results\n",
    "# print(f\"APCER: {apcer*100:.8f}, BPCER: {bpcer*100:.8f}, ACER: {acer*100:.8f}\")\n",
    "print(f\"APCER, BPCER, ACER\")\n",
    "print(f\"{apcer*100:.8f}, {bpcer*100:.8f}, {acer*100:.8f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228adb96-fca4-478e-a02f-17168f313a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2521a2a-ec72-4881-8de8-85df1279abee",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Eval results Hybrid (Img+Depth) Models ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ebd40f8-d497-41f4-885a-66d9c41d5bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and setup\n",
    "criterion = nn.CrossEntropyLoss()  # For final classification\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "device = torch.device('cuda:1')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    AdaptiveCenterCropAndResize((224, 224)),  # Adaptive crop, resize, and convert to tensor\n",
    "    # transforms.ToPILImage(),\n",
    "    # transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "###################################################\n",
    "img_dataset_path = '/home/muhammad_jabbar/face_PAD/datasets/Rose_Youtu'\n",
    "img_datasets = ['/test']\n",
    "\n",
    "depth_dataset_path = '/home/muhammad_jabbar/face_PAD/datasets/Rose_Youtu_depth_mp4/DepthAnythingV2_Base'\n",
    "depth_datasets = ['/test']\n",
    "###################################################\n",
    "\n",
    "# mobilenet_v3_large hybrid models checkpoints\n",
    "chan4_chkpt_paths = [\n",
    "    # '/home/muhammad_jabbar/face_PAD/logs/log_003_20241212_090445/checkpoints/best_model.pth', # RA\n",
    "    # '/home/muhammad_jabbar/face_PAD/logs/log_002_20241211_172816/checkpoints/best_model.pth', # RM\n",
    "    # '/home/muhammad_jabbar/face_PAD/logs/log_001_20241211_172747/checkpoints/best_model.pth', # RY\n",
    "    # '/home/muhammad_jabbar/face_PAD/logs/log_010_20241215_124956/checkpoints/best_model.pth', # OULU_NPU\n",
    "    # '/home/muhammad_jabbar/face_PAD/logs/ouluNPU_ProtocolWise/log_003_20250130_085327/checkpoints/best_model.pth', # OULU-NPU Protocol 1\n",
    "    # '/home/muhammad_jabbar/face_PAD/logs/ouluNPU_ProtocolWise/log_004_20250130_085405/checkpoints/best_model.pth', # OULU-NPU Protocol 2\n",
    "\n",
    "    ## Mobilenet_v3_large, no pretrained weights\n",
    "    # '/data/muhammad_jabbar/datasets/CNN_FacePAD_Oulu_NPU_MobileNetv3Large_NoWeights_train/logs/4ch_model/log_001_20250211_083818/checkpoints/best_model.pth', # OULU-NPU Protocol-All\n",
    "    # '/data/muhammad_jabbar/datasets/CNN_FacePAD_Oulu_NPU_MobileNetv3Large_NoWeights_train/logs/4ch_model/log_002_20250211_083914/checkpoints/best_model.pth', # OULU-NPU Protocol-1\n",
    "    # '/data/muhammad_jabbar/datasets/CNN_FacePAD_Oulu_NPU_MobileNetv3Large_NoWeights_train/logs/4ch_model/log_003_20250211_083939/checkpoints/best_model.pth', # OULU-NPU Protocol-2\n",
    "    '/home/muhammad_jabbar/face_PAD/RY_20250227/CNN_FacePAD_RY_MobileNetv3Large_NoWeights_train/logs/log_003_20250227_143850/checkpoints/best_model.pth', # RY\n",
    "]\n",
    "\n",
    "featcon_chkpt_paths = [\n",
    "    # '/home/muhammad_jabbar/face_PAD/logs/log_004_20241212_103930/checkpoints/best_model.pth', # RA\n",
    "    # '/home/muhammad_jabbar/face_PAD/logs/log_006_20241212_105604/checkpoints/best_model.pth', # RM\n",
    "    # '/home/muhammad_jabbar/face_PAD/logs/log_005_20241212_105409/checkpoints/best_model.pth', # RY\n",
    "    # '/home/muhammad_jabbar/face_PAD/logs/log_011_20241215_125132/checkpoints/best_model.pth', # OULU_NPU\n",
    "    # '/home/muhammad_jabbar/face_PAD/logs/ouluNPU_ProtocolWise/log_005_20250130_144609/checkpoints/best_model.pth',# OULU-NPU Protocol 1\n",
    "    # '/home/muhammad_jabbar/face_PAD/logs/ouluNPU_ProtocolWise/log_006_20250130_144636/checkpoints/best_model.pth', # OULU-NPU Protocol 2\n",
    "\n",
    "    ## Mobilenet_v3_large, no pretrained weights\n",
    "    # '/data/muhammad_jabbar/datasets/CNN_FacePAD_Oulu_NPU_MobileNetv3Large_NoWeights_train/logs/sepbr_model/log_001_20250211_084223/checkpoints/best_model.pth', # OULU-NPU Protocol-All\n",
    "    # '/data/muhammad_jabbar/datasets/CNN_FacePAD_Oulu_NPU_MobileNetv3Large_NoWeights_train/logs/sepbr_model/log_002_20250211_084319/checkpoints/best_model.pth', # OULU-NPU Protocol-1\n",
    "    # '/data/muhammad_jabbar/datasets/CNN_FacePAD_Oulu_NPU_MobileNetv3Large_NoWeights_train/logs/sepbr_model/log_003_20250211_084432/checkpoints/best_model.pth', # OULU-NPU Protocol-2\n",
    "    '/home/muhammad_jabbar/face_PAD/RY_20250227/CNN_FacePAD_RY_MobileNetv3Large_NoWeights_train/logs/log_004_20250227_144117/checkpoints/best_model.pth', # RY\n",
    "]\n",
    "\n",
    "aux_chkpt_paths = [\n",
    "    # '/home/muhammad_jabbar/face_PAD/logs/log_008_20241212_132803/checkpoints/best_model.pth', # RA\n",
    "    # '/home/muhammad_jabbar/face_PAD/logs/log_007_20241212_122848/checkpoints/best_model.pth', # RM\n",
    "    # '/home/muhammad_jabbar/face_PAD/logs/log_009_20241212_142559/checkpoints/best_model.pth', # RY\n",
    "    # '/home/muhammad_jabbar/face_PAD/logs/log_012_20241216_085001/checkpoints/best_model.pth', # OULU_NPU\n",
    "    # '/home/muhammad_jabbar/face_PAD/logs/ouluNPU_ProtocolWise/log_007_20250130_144824/checkpoints/best_model.pth',# OULU-NPU Protocol 1\n",
    "    # '/home/muhammad_jabbar/face_PAD/logs/ouluNPU_ProtocolWise/log_008_20250130_144840/checkpoints/best_model.pth', # OULU-NPU Protocol 2\n",
    "\n",
    "    ## Mobilenet_v3_large, no pretrained weights\n",
    "    # '/data/muhammad_jabbar/datasets/CNN_FacePAD_Oulu_NPU_MobileNetv3Large_NoWeights_train/logs/aux_model/log_001_20250211_154651/checkpoints/best_model.pth', # OULU-NPU Protocol-All\n",
    "    # '/data/muhammad_jabbar/datasets/CNN_FacePAD_Oulu_NPU_MobileNetv3Large_NoWeights_train/logs/aux_model/log_002_20250211_154756/checkpoints/best_model.pth', # OULU-NPU Protocol-1\n",
    "    # '/data/muhammad_jabbar/datasets/CNN_FacePAD_Oulu_NPU_MobileNetv3Large_NoWeights_train/logs/aux_model/log_003_20250211_154819/checkpoints/best_model.pth', # OULU-NPU Protocol-2\n",
    "    '/home/muhammad_jabbar/face_PAD/RY_20250227/CNN_FacePAD_RY_MobileNetv3Large_NoWeights_train/logs/log_005_20250227_144306/checkpoints/best_model.pth', # RY\n",
    "]\n",
    "\n",
    "\n",
    "chkpt_paths = [chan4_chkpt_paths, featcon_chkpt_paths, aux_chkpt_paths]\n",
    "\n",
    "###################################################\n",
    "\n",
    "# Instantiate the model\n",
    "model_hybridmobil_4chan = MultiChannelCNN(num_classes=2).to(device)\n",
    "model_hybridmobil_featcon = DualBranchMobileNet(num_classes=2).to(device)\n",
    "model_hybridmobil_aux = DepthAuxiliaryMobileNet(num_classes=2).to(device)\n",
    "\n",
    "modelz = [model_hybridmobil_4chan, model_hybridmobil_featcon, model_hybridmobil_aux]\n",
    "model_names = ['model_hybridmobil_4chan', 'model_hybridmobil_featcon', 'model_hybridmobil_aux']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edd98358-69ec-49e8-9afd-8d6969417ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "\n",
      "Model: model_hybridmobil_4chan\n",
      "Trained model checkpoint path : /home/muhammad_jabbar/face_PAD/RY_20250227/CNN_FacePAD_RY_MobileNetv3Large_NoWeights_train/logs/log_003_20250227_143850/checkpoints/best_model.pth\n",
      "Test img dataet path # : /home/muhammad_jabbar/face_PAD/datasets/Rose_Youtu/test\n",
      "Test depth dataet path # : /home/muhammad_jabbar/face_PAD/datasets/Rose_Youtu_depth_mp4/DepthAnythingV2_Base/test\n",
      "Test samples: 1748\n",
      "Test dataet batches # : 7\n",
      "\n",
      "\n",
      "--- Evaluation Summary ---\n",
      "HTER (%), AUC-ROC, Test Accuracy (%)\n",
      "11.9616, 0.9384, 88.4439\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "Model: model_hybridmobil_featcon\n",
      "Trained model checkpoint path : /home/muhammad_jabbar/face_PAD/RY_20250227/CNN_FacePAD_RY_MobileNetv3Large_NoWeights_train/logs/log_004_20250227_144117/checkpoints/best_model.pth\n",
      "Test img dataet path # : /home/muhammad_jabbar/face_PAD/datasets/Rose_Youtu/test\n",
      "Test depth dataet path # : /home/muhammad_jabbar/face_PAD/datasets/Rose_Youtu_depth_mp4/DepthAnythingV2_Base/test\n",
      "Test samples: 1748\n",
      "Test dataet batches # : 7\n",
      "\n",
      "\n",
      "--- Evaluation Summary ---\n",
      "HTER (%), AUC-ROC, Test Accuracy (%)\n",
      "11.3954, 0.9494, 90.3318\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "Model: model_hybridmobil_aux\n",
      "Trained model checkpoint path : /home/muhammad_jabbar/face_PAD/RY_20250227/CNN_FacePAD_RY_MobileNetv3Large_NoWeights_train/logs/log_005_20250227_144306/checkpoints/best_model.pth\n",
      "Test img dataet path # : /home/muhammad_jabbar/face_PAD/datasets/Rose_Youtu/test\n",
      "Test depth dataet path # : /home/muhammad_jabbar/face_PAD/datasets/Rose_Youtu_depth_mp4/DepthAnythingV2_Base/test\n",
      "Test samples: 1748\n",
      "Test dataet batches # : 7\n",
      "\n",
      "\n",
      "--- Evaluation Summary ---\n",
      "HTER (%), AUC-ROC, Test Accuracy (%)\n",
      "12.8539, 0.9405, 89.7025\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "hters = []\n",
    "auc_rocs = []\n",
    "test_accs = []\n",
    "\n",
    "num_frames = 1\n",
    "batch_size=256\n",
    "\n",
    "n_split = 1\n",
    "\n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "for i, model in enumerate(modelz):\n",
    "    for j, chkpt_path in enumerate(chkpt_paths[i]):\n",
    "        for k, img_dataset in enumerate(img_datasets):\n",
    "\n",
    "            test_dataset = VideoDataset(\n",
    "                orig_root_dir = img_dataset_path + img_dataset,\n",
    "                depth_root_dir = depth_dataset_path + depth_datasets[k],\n",
    "                transform=transform,\n",
    "                num_frames=num_frames,\n",
    "                is_train=False,\n",
    "            )\n",
    "\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "            checkpoint = torch.load(chkpt_path, weights_only=True)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "            print()            \n",
    "            print(f'Model: {model_names[i]}')\n",
    "            print(f'Trained model checkpoint path : {chkpt_path}')\n",
    "            print(f'Test img dataet path # : {img_dataset_path + img_dataset}')\n",
    "            print(f'Test depth dataet path # : {depth_dataset_path + depth_datasets[k]}')\n",
    "            print(f\"Test samples: {len(test_dataset)}\")\n",
    "            print(f'Test dataet batches # : {len(test_loader)}')\n",
    "            print()\n",
    "            \n",
    "            # Assuming `evaluate` returns the results dictionary\n",
    "            results = evaluate_all(model, test_loader, criterion)\n",
    "\n",
    "            # Generate the summary and plot graphs\n",
    "            hter, auc_roc, test_acc = generate_evaluation_summary(results)\n",
    "            hters.append(hter)\n",
    "            auc_rocs.append(auc_roc)\n",
    "            test_accs.append(test_acc)\n",
    "            \n",
    "            print()\n",
    "            print('-----------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "047aefe1-f697-4bcb-8c36-febae303a69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "\n",
      "Model: model_hybridmobil_4chan\n",
      "Trained model checkpoint path : /data/muhammad_jabbar/datasets/CNN_FacePAD_Oulu_NPU_MobileNetv3Large_NoWeights_train/logs/4ch_model/log_001_20250211_083818/checkpoints/best_model.pth\n",
      "Test img dataet path # : /data/muhammad_jabbar/datasets/Oulu_NPU/Test_files\n",
      "Test depth dataet path # : /data/muhammad_jabbar/datasets/Oulu_NPU_depth_mp4/Test_files\n",
      "OULU-NPU Protocol File List # : /data/muhammad_jabbar/datasets/Oulu_NPU/Baseline/Protocol_all/Test.txt\n",
      "OULU-NPU Protocol # : all\n",
      "Test samples: 1800\n",
      "Test dataet batches # : 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [05:50<00:00, 43.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APCER, BPCER, ACER\n",
      "4.86111111, 20.83333333, 12.84722222\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "Model: model_hybridmobil_4chan\n",
      "Trained model checkpoint path : /data/muhammad_jabbar/datasets/CNN_FacePAD_Oulu_NPU_MobileNetv3Large_NoWeights_train/logs/4ch_model/log_002_20250211_083914/checkpoints/best_model.pth\n",
      "Test img dataet path # : /data/muhammad_jabbar/datasets/Oulu_NPU/Test_files\n",
      "Test depth dataet path # : /data/muhammad_jabbar/datasets/Oulu_NPU_depth_mp4/Test_files\n",
      "OULU-NPU Protocol File List # : /data/muhammad_jabbar/datasets/Oulu_NPU/Baseline/Protocol_1/Test.txt\n",
      "OULU-NPU Protocol # : 1\n",
      "Test samples: 600\n",
      "Test dataet batches # : 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [02:33<00:00, 51.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APCER, BPCER, ACER\n",
      "0.00000000, 100.00000000, 50.00000000\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "Model: model_hybridmobil_4chan\n",
      "Trained model checkpoint path : /data/muhammad_jabbar/datasets/CNN_FacePAD_Oulu_NPU_MobileNetv3Large_NoWeights_train/logs/4ch_model/log_003_20250211_083939/checkpoints/best_model.pth\n",
      "Test img dataet path # : /data/muhammad_jabbar/datasets/Oulu_NPU/Test_files\n",
      "Test depth dataet path # : /data/muhammad_jabbar/datasets/Oulu_NPU_depth_mp4/Test_files\n",
      "OULU-NPU Protocol File List # : /data/muhammad_jabbar/datasets/Oulu_NPU/Baseline/Protocol_2/Test.txt\n",
      "OULU-NPU Protocol # : 2\n",
      "Test samples: 1080\n",
      "Test dataet batches # : 5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [04:31<00:00, 54.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APCER, BPCER, ACER\n",
      "9.16666667, 15.00000000, 12.08333333\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "Model: model_hybridmobil_featcon\n",
      "Trained model checkpoint path : /data/muhammad_jabbar/datasets/CNN_FacePAD_Oulu_NPU_MobileNetv3Large_NoWeights_train/logs/sepbr_model/log_001_20250211_084223/checkpoints/best_model.pth\n",
      "Test img dataet path # : /data/muhammad_jabbar/datasets/Oulu_NPU/Test_files\n",
      "Test depth dataet path # : /data/muhammad_jabbar/datasets/Oulu_NPU_depth_mp4/Test_files\n",
      "OULU-NPU Protocol File List # : /data/muhammad_jabbar/datasets/Oulu_NPU/Baseline/Protocol_all/Test.txt\n",
      "OULU-NPU Protocol # : all\n",
      "Test samples: 1800\n",
      "Test dataet batches # : 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [05:44<00:00, 43.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APCER, BPCER, ACER\n",
      "1.52777778, 19.16666667, 10.34722222\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "Model: model_hybridmobil_featcon\n",
      "Trained model checkpoint path : /data/muhammad_jabbar/datasets/CNN_FacePAD_Oulu_NPU_MobileNetv3Large_NoWeights_train/logs/sepbr_model/log_002_20250211_084319/checkpoints/best_model.pth\n",
      "Test img dataet path # : /data/muhammad_jabbar/datasets/Oulu_NPU/Test_files\n",
      "Test depth dataet path # : /data/muhammad_jabbar/datasets/Oulu_NPU_depth_mp4/Test_files\n",
      "OULU-NPU Protocol File List # : /data/muhammad_jabbar/datasets/Oulu_NPU/Baseline/Protocol_1/Test.txt\n",
      "OULU-NPU Protocol # : 1\n",
      "Test samples: 600\n",
      "Test dataet batches # : 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:22<00:00, 27.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APCER, BPCER, ACER\n",
      "0.00000000, 96.66666667, 48.33333333\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "Model: model_hybridmobil_featcon\n",
      "Trained model checkpoint path : /data/muhammad_jabbar/datasets/CNN_FacePAD_Oulu_NPU_MobileNetv3Large_NoWeights_train/logs/sepbr_model/log_003_20250211_084432/checkpoints/best_model.pth\n",
      "Test img dataet path # : /data/muhammad_jabbar/datasets/Oulu_NPU/Test_files\n",
      "Test depth dataet path # : /data/muhammad_jabbar/datasets/Oulu_NPU_depth_mp4/Test_files\n",
      "OULU-NPU Protocol File List # : /data/muhammad_jabbar/datasets/Oulu_NPU/Baseline/Protocol_2/Test.txt\n",
      "OULU-NPU Protocol # : 2\n",
      "Test samples: 1080\n",
      "Test dataet batches # : 5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [03:50<00:00, 46.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APCER, BPCER, ACER\n",
      "15.83333333, 6.38888889, 11.11111111\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "Model: model_hybridmobil_aux\n",
      "Trained model checkpoint path : /data/muhammad_jabbar/datasets/CNN_FacePAD_Oulu_NPU_MobileNetv3Large_NoWeights_train/logs/aux_model/log_001_20250211_154651/checkpoints/best_model.pth\n",
      "Test img dataet path # : /data/muhammad_jabbar/datasets/Oulu_NPU/Test_files\n",
      "Test depth dataet path # : /data/muhammad_jabbar/datasets/Oulu_NPU_depth_mp4/Test_files\n",
      "OULU-NPU Protocol File List # : /data/muhammad_jabbar/datasets/Oulu_NPU/Baseline/Protocol_all/Test.txt\n",
      "OULU-NPU Protocol # : all\n",
      "Test samples: 1800\n",
      "Test dataet batches # : 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [06:14<00:00, 46.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APCER, BPCER, ACER\n",
      "5.83333333, 27.77777778, 16.80555556\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "Model: model_hybridmobil_aux\n",
      "Trained model checkpoint path : /data/muhammad_jabbar/datasets/CNN_FacePAD_Oulu_NPU_MobileNetv3Large_NoWeights_train/logs/aux_model/log_002_20250211_154756/checkpoints/best_model.pth\n",
      "Test img dataet path # : /data/muhammad_jabbar/datasets/Oulu_NPU/Test_files\n",
      "Test depth dataet path # : /data/muhammad_jabbar/datasets/Oulu_NPU_depth_mp4/Test_files\n",
      "OULU-NPU Protocol File List # : /data/muhammad_jabbar/datasets/Oulu_NPU/Baseline/Protocol_1/Test.txt\n",
      "OULU-NPU Protocol # : 1\n",
      "Test samples: 600\n",
      "Test dataet batches # : 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:22<00:00, 27.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APCER, BPCER, ACER\n",
      "0.41666667, 99.16666667, 49.79166667\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "Model: model_hybridmobil_aux\n",
      "Trained model checkpoint path : /data/muhammad_jabbar/datasets/CNN_FacePAD_Oulu_NPU_MobileNetv3Large_NoWeights_train/logs/aux_model/log_003_20250211_154819/checkpoints/best_model.pth\n",
      "Test img dataet path # : /data/muhammad_jabbar/datasets/Oulu_NPU/Test_files\n",
      "Test depth dataet path # : /data/muhammad_jabbar/datasets/Oulu_NPU_depth_mp4/Test_files\n",
      "OULU-NPU Protocol File List # : /data/muhammad_jabbar/datasets/Oulu_NPU/Baseline/Protocol_2/Test.txt\n",
      "OULU-NPU Protocol # : 2\n",
      "Test samples: 1080\n",
      "Test dataet batches # : 5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [02:18<00:00, 27.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APCER, BPCER, ACER\n",
      "15.83333333, 8.61111111, 12.22222222\n",
      "\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### OULU APCER, BPCER, and ACER Calculation ###\n",
    "import oulumetrics\n",
    "\n",
    "num_frames = 1\n",
    "batch_size=256\n",
    "\n",
    "n_split = 1\n",
    "\n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "for i, model in enumerate(modelz):\n",
    "    for j, chkpt_path in enumerate(chkpt_paths[i]):\n",
    "\n",
    "        protocol = 'all' if j==0 else str(j)\n",
    "\n",
    "        if protocol=='3' or protocol=='4':\n",
    "            protocol_flist = f'/data/muhammad_jabbar/datasets/Oulu_NPU/Baseline/Protocol_{protocol}/Test_{n_split}.txt'\n",
    "        else:\n",
    "            protocol_flist = f'/data/muhammad_jabbar/datasets/Oulu_NPU/Baseline/Protocol_{protocol}/Test.txt'\n",
    "\n",
    "        for k, img_dataset in enumerate(img_datasets):\n",
    "            \n",
    "            test_dataset = VideoDataset(\n",
    "                orig_root_dir = img_dataset_path + img_dataset,\n",
    "                depth_root_dir = depth_dataset_path + depth_datasets[k],\n",
    "                file_list_path = protocol_flist,\n",
    "                transform=transform,\n",
    "                num_frames=num_frames,\n",
    "                is_train=False,\n",
    "                protocol = protocol\n",
    "            )\n",
    "\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "            checkpoint = torch.load(chkpt_path, weights_only=True)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "            print()            \n",
    "            print(f'Model: {model_names[i]}')\n",
    "            print(f'Trained model checkpoint path : {chkpt_path}')\n",
    "            print(f'Test img dataet path # : {img_dataset_path + img_dataset}')\n",
    "            print(f'Test depth dataet path # : {depth_dataset_path + depth_datasets[k]}')\n",
    "            print(f'OULU-NPU Protocol File List # : {protocol_flist}')\n",
    "            print(f'OULU-NPU Protocol # : {protocol}')\n",
    "            print(f\"Test samples: {len(test_dataset)}\")\n",
    "            print(f'Test dataet batches # : {len(test_loader)}')\n",
    "            print()\n",
    "            \n",
    "            y_attack_types = []\n",
    "            y_pred = []\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in tqdm(test_loader):\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                    # Perform inference\n",
    "                    outputs = model(inputs)\n",
    "                    probabilities = torch.softmax(outputs, dim=1)  # Assuming outputs are logits\n",
    "                    predictions = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "                    # Append ground truth and predictions to lists\n",
    "                    y_attack_types.extend(labels.cpu().numpy())\n",
    "                    y_pred.extend(predictions.cpu().numpy())\n",
    "\n",
    "            # Convert lists to numpy arrays\n",
    "            y_attack_types = np.array(y_attack_types)\n",
    "            y_pred = np.array(y_pred)\n",
    "\n",
    "            apcer, bpcer, acer = oulumetrics.calculate_metrics(y_attack_types, y_pred)\n",
    "\n",
    "            # Print the results\n",
    "            # print(f\"APCER: {apcer*100:.8f}, BPCER: {bpcer*100:.8f}, ACER: {acer*100:.8f}\")                    \n",
    "            print(f\"APCER, BPCER, ACER\")                    \n",
    "            print(f\"{apcer*100:.8f}, {bpcer*100:.8f}, {acer*100:.8f}\")                    \n",
    "            \n",
    "            print()\n",
    "            print('-----------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecf4d273-0125-4394-b076-aab148eed779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e558a19f-9452-434d-9332-7887858d6b38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39+torch",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
